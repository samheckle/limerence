{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28708153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./env/lib/python3.8/site-packages (23.0.1)\n",
      "Requirement already satisfied: install in ./env/lib/python3.8/site-packages (1.3.5)\n",
      "Requirement already satisfied: spacy in ./env/lib/python3.8/site-packages (3.5.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./env/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./env/lib/python3.8/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./env/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./env/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./env/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./env/lib/python3.8/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./env/lib/python3.8/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.8/site-packages (from spacy) (49.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./env/lib/python3.8/site-packages (from spacy) (1.10.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./env/lib/python3.8/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./env/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./env/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./env/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./env/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./env/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.8/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./env/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./env/lib/python3.8/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./env/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./env/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./env/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./env/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.8/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./env/lib/python3.8/site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (49.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./env/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./env/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./env/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./env/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "Requirement already satisfied: tracery in ./env/lib/python3.8/site-packages (0.1.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pip install -U spacy\n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "!{sys.executable} -m pip install tracery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9141715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e1c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"2try.txt\").read()\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cea62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "words = [w for w in list(doc) if w.is_alpha]\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "entities = list(doc.ents)\n",
    "times = [e for e in entities if e.label_ == \"TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66306086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a scene\n",
      "\n",
      "suicide\n",
      "\n",
      "That\n",
      "\n",
      "the delight\n",
      "\n",
      "who\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for item in random.sample(noun_chunks, 5):\n",
    "    print(item.text.strip().replace(\"\\n\", \" \"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae02b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the night\n",
      "a few seconds\n",
      "night\n",
      "all night\n",
      "an intelligible din\n",
      "every other night\n",
      "Tonight\n",
      "a hundred nights\n",
      "That night\n",
      "This morning\n",
      "a difficult evening\n",
      "four hours later\n",
      "tonight\n",
      "the morning\n",
      "this morning\n",
      "last night\n",
      "night\n",
      "just last night\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(times, 18): # change \"times\" to \"people\" or \"locations\" to sample those lists\n",
    "    print(item.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5077cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in list(st)]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b9d97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_phrases = []\n",
    "for word in doc:\n",
    "    if word.dep_ == 'prep':\n",
    "        prep_phrases.append(flatten_subtree(word.subtree).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f446821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For the system',\n",
       " 'in',\n",
       " 'of words and of arguments',\n",
       " 'without ever hoping to be able to catch up, to coin\\xad cide with him',\n",
       " 'of phrase, a kind of verse, refrain, or cantilla\\xad tion which articulates it in the darkness',\n",
       " 'in the \"nature\" of amorous madness to pass',\n",
       " 'either in the form of bore\\xad dom',\n",
       " 'as a type ( in Nietzschean SCHELLING',\n",
       " 'of \"crass materialism',\n",
       " 'of reality lands on me',\n",
       " 'against insignificance',\n",
       " 'over',\n",
       " 'as evasions',\n",
       " 'in le.  Agony angoisse / anxiety The amorous subject',\n",
       " 'with a view to having the \"last word',\n",
       " 'in the same way',\n",
       " 'in my body',\n",
       " 'by a specifically amorous perversion',\n",
       " 'by \"innocently\" furnishing commonplace information about the loved being,',\n",
       " 'through his mouth',\n",
       " 'with the chatter of the women in the drugstore who are delaying my return to the instrument to which I am subjugated',\n",
       " 'to the amorous crisis',\n",
       " 'with pain',\n",
       " 'in a single direction',\n",
       " 'of spoiling',\n",
       " 'with the bad one:',\n",
       " 'for the other',\n",
       " 'of God',\n",
       " 'of the Via dei Condotti, where ten years ago I had bought a silk shirt and thin summer socks',\n",
       " 'in love',\n",
       " 'on the reservoir (the thesaurus?) of figures, depend\\xad ing on the needs, the injunctions, or the pleasures of his image-repertoire',\n",
       " 'of Charlotte',\n",
       " 'of difference',\n",
       " 'in a certain order around one point',\n",
       " 'in set\\xad',\n",
       " 'in bundles of sentences',\n",
       " 'with delight',\n",
       " 'to no ritual',\n",
       " 'of the others from which I am ex\\xad cluded',\n",
       " 'on those around you',\n",
       " \"of the beloved's absence\",\n",
       " 'by an oath of fidelity',\n",
       " 'to various degrees',\n",
       " 'of that coalescent sub\\xad stance',\n",
       " 'during the day',\n",
       " 'without warning',\n",
       " 'of Hubert',\n",
       " 'like a workman of the electronic age, or',\n",
       " 'into mourning for his analyst',\n",
       " 'of the Image-repertoire',\n",
       " 'in order to fall in love',\n",
       " \"in order to be sure of keeping his best friend's wife from knowing that he loves her passionately\",\n",
       " 'upon love,',\n",
       " 'like a syncope in the lovely phrase of the loved being',\n",
       " 'to style',\n",
       " 'of person (love confides',\n",
       " 'of holding a cigarette',\n",
       " 'of some serious and ab\\xad stract branch of learning',\n",
       " 'of a love-sick subject to that of an in\\xad mate of Dachau',\n",
       " 'around me',\n",
       " 'in the fairy tales',\n",
       " 'what to Z',\n",
       " 'at once',\n",
       " 'with repressed feeling',\n",
       " 'in the mulling over of images',\n",
       " 'on the other side of the glass',\n",
       " 'in the grave',\n",
       " 'of the ego\" (Outline of Psychoanalysis)',\n",
       " 'of communi\\xad cation',\n",
       " 'of despair',\n",
       " 'in the Monastery of Sainte-Marie-des-Bois',\n",
       " 'with someone who knows',\n",
       " 'beneath the monstrous discourse of the amorous subject',\n",
       " 'in',\n",
       " 'through all the meanderings of my amorous history',\n",
       " 'like everyone else: to be jealous',\n",
       " 'of the image',\n",
       " 'for Imitation, Representation, and Analogy',\n",
       " 'in the straining body',\n",
       " 'within myself',\n",
       " 'from hallucinations',\n",
       " 'of the amorous body, which is a body in liquid expansion, a bathed body: to weep together, to flow together',\n",
       " 'under a zodiacal sign which excludes jealousy',\n",
       " 'to coldness',\n",
       " 'than all those who simply do not know this about me',\n",
       " 'in the dark',\n",
       " 'of Gradiva',\n",
       " 'to the \"inner storm\"',\n",
       " 'into sympathy',\n",
       " 'to the toilet',\n",
       " 'of reality',\n",
       " 'of language',\n",
       " 'in a perfrct indifTercntia1i0n',\n",
       " 'upon whom',\n",
       " 'by which',\n",
       " 'of my devo\\xad',\n",
       " 'over',\n",
       " 'in this kind of love',\n",
       " 'with Coluche, the res\\xad taurant, the painter, the Piazza de! Popolo',\n",
       " 'En sa moytie, ma moytie je']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(prep_phrases, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f49330",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "            for word in doc if word.dep_ in ('nsubj', 'nsubjpass')]\n",
    "past_tense_verbs = [word.text for word in words if word.tag_ == 'VBD' and word.lemma_ != 'be']\n",
    "adjectives = [word.text for word in words if word.tag_.startswith('JJ')]\n",
    "nouns = [word.text for word in words if word.tag_.startswith('NN')]\n",
    "prep_phrases = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "                for word in doc if word.dep_ == 'prep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a703e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracery\n",
    "import textwrap\n",
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13da7246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m grammar \u001b[38;5;241m=\u001b[39m tracery\u001b[38;5;241m.\u001b[39mGrammar(rules)\n\u001b[1;32m     29\u001b[0m grammar\u001b[38;5;241m.\u001b[39madd_modifiers(base_english)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#origin#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:244\u001b[0m, in \u001b[0;36mGrammar.flatten\u001b[0;34m(self, rule, allow_escape_chars)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten\u001b[39m(\u001b[38;5;28mself\u001b[39m, rule, allow_escape_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 244\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_escape_chars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m root\u001b[38;5;241m.\u001b[39mfinished_text\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:237\u001b[0m, in \u001b[0;36mGrammar.expand\u001b[0;34m(self, rule, allow_escape_chars)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand\u001b[39m(\u001b[38;5;28mself\u001b[39m, rule, allow_escape_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    236\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_root(rule)\n\u001b[0;32m--> 237\u001b[0m     \u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_escape_chars:\n\u001b[1;32m    239\u001b[0m         root\u001b[38;5;241m.\u001b[39mclear_escape_chars()\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:63\u001b[0m, in \u001b[0;36mNode.expand\u001b[0;34m(self, prevent_recursion)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Types of nodes\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# -1: raw, needs parsing\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#  0: Plaintext\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#  2: Action (\"[pushTarget:pushRule], [pushTarget:POP]\",\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#     more in the future)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_children\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevent_recursion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:46\u001b[0m, in \u001b[0;36mNode.expand_children\u001b[0;34m(self, child_rule, prevent_recursion)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prevent_recursion:\n\u001b[0;32m---> 46\u001b[0m             \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevent_recursion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mfinished_text\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:84\u001b[0m, in \u001b[0;36mNode.expand\u001b[0;34m(self, prevent_recursion)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m     82\u001b[0m selected_rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar\u001b[38;5;241m.\u001b[39mselect_rule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbol, \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     83\u001b[0m                                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevent_recursion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# apply modifiers\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodifiers:\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:46\u001b[0m, in \u001b[0;36mNode.expand_children\u001b[0;34m(self, child_rule, prevent_recursion)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prevent_recursion:\n\u001b[0;32m---> 46\u001b[0m             \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevent_recursion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mfinished_text\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:84\u001b[0m, in \u001b[0;36mNode.expand\u001b[0;34m(self, prevent_recursion)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m     82\u001b[0m selected_rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar\u001b[38;5;241m.\u001b[39mselect_rule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbol, \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     83\u001b[0m                                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevent_recursion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# apply modifiers\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodifiers:\n",
      "    \u001b[0;31m[... skipping similar frames: Node.expand at line 84 (1 times), Node.expand_children at line 46 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:46\u001b[0m, in \u001b[0;36mNode.expand_children\u001b[0;34m(self, child_rule, prevent_recursion)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prevent_recursion:\n\u001b[0;32m---> 46\u001b[0m             \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevent_recursion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mfinished_text\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:84\u001b[0m, in \u001b[0;36mNode.expand\u001b[0;34m(self, prevent_recursion)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m     82\u001b[0m selected_rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrammar\u001b[38;5;241m.\u001b[39mselect_rule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbol, \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     83\u001b[0m                                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevent_recursion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# apply modifiers\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodifiers:\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:40\u001b[0m, in \u001b[0;36mNode.expand_children\u001b[0;34m(self, child_rule, prevent_recursion)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchild_rule \u001b[38;5;241m=\u001b[39m child_rule\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchild_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     sections, errors \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mextend(errors)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, section \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sections):\n",
      "File \u001b[0;32m~/dev/residency/spring-2023/limerence/text-gen/env/lib/python3.8/site-packages/tracery/__init__.py:323\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(rule)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rule):\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m escaped:\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m:\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_tag:\n\u001b[1;32m    325\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m i:\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got str)"
     ]
    }
   ],
   "source": [
    "rules = {\n",
    "    \"origin\": [\n",
    "        #\"#subject.capitalize# #predicate#.\",\n",
    "        #\"#subject.capitalize# #predicate#.\",\n",
    "        \"#prepphrase.capitalize#, you #predicate#.\"\n",
    "    ],\n",
    "    \"predicate\": [\n",
    "        \"#verb#\",\n",
    "        \"#verb# #nounphrase#\",\n",
    "        \"#verb# #prepphrase#\"\n",
    "    ],\n",
    "    \"nounphrase\": [\n",
    "        \"the #noun#\",\n",
    "        \"the #adj# #noun#\",\n",
    "        \"the #noun# #prepphrase#\",\n",
    "        \"the #noun# and the #noun#\",\n",
    "        \"#noun.a#\",\n",
    "        \"#noun-chunks.a#\",\n",
    "        \"#adj.a# #noun#\",\n",
    "        \"the #noun# that #predicate#\"\n",
    "    ],\n",
    "    \"verb\": past_tense_verbs,\n",
    "    \"noun\": nouns,\n",
    "    \"noun-chunks\": noun_chunks,\n",
    "    \"adj\": adjectives,\n",
    "    \"prepphrase\": prep_phrases\n",
    "}\n",
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)\n",
    "grammar.flatten(\"#origin#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "output = ''\n",
    "for i in range(12):\n",
    "    output += grammar.flatten(\"#origin#\")\n",
    "    output += \" \"\n",
    "# output = \" \".join([grammar.flatten(\"#origin#\") for i in range(12)])\n",
    "# print(fill(output, 60))\n",
    "print(textwrap.fill(output, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "output = \" \".join([grammar.flatten(\"#origin#\") for i in range(12)])\n",
    "#print(fill(output, 60))\n",
    "print(textwrap.fill(output, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad681df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "    \"origin\": [\n",
    "#         \"#prepphrase#\",\n",
    "#         \"#predicate#.a#\",\n",
    "        \"#prepphrase.capitalize#, you #predicate#.\",\n",
    "        \"You #predicate# #prepphrase#.\"\n",
    "    ],\n",
    "    \"predicate\": [\n",
    "        \"#verb#\",\n",
    "        \"#verb# #nounphrase#\",\n",
    "        \"#verb# #prepphrase#\"\n",
    "    ],\n",
    "    \"nounphrase\": [\n",
    "        \"the #noun#\",\n",
    "        \"the #adj# #noun#\",\n",
    "        \"the #noun# #prepphrase#\",\n",
    "        \"the #noun# and the #noun#\",\n",
    "        \"#noun.a#\",\n",
    "        \"#noun-chunks.a#\",\n",
    "        \"#adj.a# #noun#\",\n",
    "        \"the #noun# that #predicate#\"\n",
    "    ],\n",
    "    \"verb\": past_tense_verbs,\n",
    "    \"noun\": nouns,\n",
    "    \"noun-chunks\": noun_chunks,\n",
    "    \"adj\": adjectives,\n",
    "    \"prepphrase\": prep_phrases\n",
    "}\n",
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)\n",
    "grammar.flatten(\"#origin#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78725029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from textwrap import fill\n",
    "output = \" \".join([grammar.flatten(\"#origin#\") for i in range(12)])\n",
    "\n",
    "export = print(textwrap.fill(output, 60))\n",
    "for line in export:\n",
    "    words = line.split() \n",
    "    outputs = [] \n",
    "    sel = random.sample(words, random.randint(0,len(words)))\n",
    "    \n",
    "    for item in words:\n",
    "        if item in sel:\n",
    "            outputs.append(item)\n",
    "        for char in item:\n",
    "            outputs.append('.')\n",
    "    print(textwrap.fill(''.join(export), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86297b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from textwrap import fill\n",
    "output = \"\\n\".join([grammar.flatten(\"#origin#\") for i in range(5)])\n",
    "print(textwrap.fill(output, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c697ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markovify\n",
      "  Using cached markovify-0.9.4-py3-none-any.whl\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode, markovify\n",
      "Successfully installed markovify-0.9.4 unidecode-1.3.6\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install markovify\n",
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87482143",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adorable is what T imagine I give.\n",
      "\n",
      "Yet the initial scene during which I am indefectibly myself, and it is proper to the foreclosure of any will-to-possess.\n",
      "\n",
      "I am excluded from it gently, initially conforms to his weakness, not to be perverse, it again becomes imaginary, I return to the hotel alone; the other hand, I am both too big and too weak for writing: I am suffering an amorous sadness, this void requires that we can be an erotic, pornographic word.\n",
      "\n",
      "Or again: my entire self is drawn, transferred to the woman he has no scientific place: I-love-you belongs neither in the room, looking embarrassed; various trivial subjects of conversation that the telephone the other side of my own body produces the incident: an evening I was ravished is merely the victim of his own, and her husband is not the screen around a secret, but, instead, a kind of evidence in which case you will be the person he loves.\n",
      "\n",
      "In those brief moments when I am sought after, surrounded, flattered.\n",
      "\n",
      "I wear myself out, I shall suffer, for I love only one.\n",
      "\n",
      "Initially it is the genesis of the Image-repertoire, its triumph.\n",
      "\n",
      "My daimon, on the other, l want the Other to speak performs the disappear­ ance of the loved being.\n",
      "\n",
      "If I could be anything but jealous.\n",
      "\n",
      "BUNUEL: The Discreet Charm of the loved object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = open(\"2try.txt\").read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d158baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "\n",
      "I take a role: I am caught up; I am caught up in a double discourse, from which I cannot escape.\n",
      "\n",
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "\n",
      "Such is amorous fatigue: a hunger not to be found, for bad humor is nothing more or less than a message.\n",
      "\n",
      "I take a role: I am my own theater.\n",
      "\n",
      "I alone have a rendezvous, I am alarmed by the slightest injuries.\n",
      "\n",
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "\n",
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "\n",
      "Such is amorous fatigue: a hunger not to be found, for bad humor is nothing more or less than a message.\n",
      "\n",
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputMarkov = open(\"outputMarkov.txt\").read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(outputMarkov)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a2c848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gift is futile is obviously not to keep myself from which I take a message.\n",
      "I am my desire, quite special as it makes me cry: I am The delight of too much or not to keep myself from which I am the regime of too much or not enough; greedy for coincidence as I play this mean, then, that my own theater.\n",
      "Such is going to be satisfied, a gaping love.\n",
      "Such is contact, sensuality: you will be touching what I have touched, a hunger not to be satisfied, a third skin unites us. it is without a rendezvous, I have a hunger not enough; greedy for myself, and I am caught up; I play this mean, then, that my own theater.\n",
      "You are everywhere, your cry means.\n",
      "What is going to keep myself from doing so, which drives me cry: I am caught up; I am The gift is without a type?\n",
      "I am caught up; I live under the regime of inclusion; in a gaping love.\n",
      "You are everywhere, your cry means.\n",
      "The gift is linked to a hunger not to be touching what your cry means.\n",
      "I am The gift is contact, sensuality: you will be found, for myself, and I have a reply.\n"
     ]
    }
   ],
   "source": [
    "outputMarkov = open(\"outputMarkov.txt\").read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(outputMarkov, state_size=1)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0351fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "I take a role: I am my own theater.\n",
      "I alone have a rendezvous, I am caught up in a double discourse, from which I cannot escape.\n",
      "I take a role: I am my own theater.\n",
      "Such a man is obviously not to be satisfied, a gaping love.\n",
      "Such is amorous fatigue: a hunger not to be found, for bad humor is nothing more or less than a message.\n",
      "I alone have a rendezvous, I am caught up in a double discourse, from which I cannot escape.\n",
      "Such is amorous fatigue: a hunger not to be found, for bad humor is nothing more or less than a message.\n"
     ]
    }
   ],
   "source": [
    "outputMarkov = open(\"outputMarkov.txt\").read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(outputMarkov)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5e82658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_model(model, n, seq):\n",
    "    # make a copy of seq and append None to the end\n",
    "    seq = list(seq[:]) + [None]\n",
    "    for i in range(len(seq)-n):\n",
    "        # tuple because we're using it as a dict key!\n",
    "        gram = tuple(seq[i:i+n])\n",
    "        next_item = seq[i+n]            \n",
    "        if gram not in model:\n",
    "            model[gram] = []\n",
    "        model[gram].append(next_item)\n",
    "\n",
    "def markov_model(n, seq):\n",
    "    model = {}\n",
    "    add_to_model(model, n, seq)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "genesis_markov_model = markov_model(3, open(\"outputMarkov.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9db62119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('I', 'alone', 'have'): ['a'],\n",
       " ('alone', 'have', 'a'): ['rendezvous,'],\n",
       " ('have', 'a', 'rendezvous,'): ['I'],\n",
       " ('a', 'rendezvous,', 'I'): ['am'],\n",
       " ('rendezvous,', 'I', 'am'): ['caught'],\n",
       " ('I', 'am', 'caught'): ['up;', 'up'],\n",
       " ('am', 'caught', 'up;'): ['I'],\n",
       " ('caught', 'up;', 'I'): ['am'],\n",
       " ('up;', 'I', 'am'): ['alarmed'],\n",
       " ('I', 'am', 'alarmed'): ['by'],\n",
       " ('am', 'alarmed', 'by'): ['the'],\n",
       " ('alarmed', 'by', 'the'): ['slightest'],\n",
       " ('by', 'the', 'slightest'): ['injuries.'],\n",
       " ('the', 'slightest', 'injuries.'): ['The'],\n",
       " ('slightest', 'injuries.', 'The'): ['delight'],\n",
       " ('injuries.', 'The', 'delight'): ['of'],\n",
       " ('The', 'delight', 'of'): ['inclusion;'],\n",
       " ('delight', 'of', 'inclusion;'): ['in'],\n",
       " ('of', 'inclusion;', 'in'): ['such'],\n",
       " ('inclusion;', 'in', 'such'): ['discussion,'],\n",
       " ('in', 'such', 'discussion,'): ['the'],\n",
       " ('such', 'discussion,', 'the'): ['object'],\n",
       " ('discussion,', 'the', 'object'): ['of'],\n",
       " ('the', 'object', 'of'): ['love...'],\n",
       " ('object', 'of', 'love...'): ['What'],\n",
       " ('of', 'love...', 'What'): ['is'],\n",
       " ('love...', 'What', 'is'): ['futile'],\n",
       " ('What', 'is', 'futile'): ['is'],\n",
       " ('is', 'futile', 'is'): ['what'],\n",
       " ('futile', 'is', 'what'): ['your'],\n",
       " ('is', 'what', 'your'): ['cry'],\n",
       " ('what', 'your', 'cry'): ['means.'],\n",
       " ('your', 'cry', 'means.'): ['You'],\n",
       " ('cry', 'means.', 'You'): ['are'],\n",
       " ('means.', 'You', 'are'): ['everywhere,'],\n",
       " ('You', 'are', 'everywhere,'): ['your'],\n",
       " ('are', 'everywhere,', 'your'): ['image'],\n",
       " ('everywhere,', 'your', 'image'): ['is'],\n",
       " ('your', 'image', 'is'): ['without'],\n",
       " ('image', 'is', 'without'): ['a'],\n",
       " ('is', 'without', 'a'): ['reply.'],\n",
       " ('without', 'a', 'reply.'): ['Such'],\n",
       " ('a', 'reply.', 'Such'): ['a'],\n",
       " ('reply.', 'Such', 'a'): ['man'],\n",
       " ('Such', 'a', 'man'): ['is'],\n",
       " ('a', 'man', 'is'): ['obviously'],\n",
       " ('man', 'is', 'obviously'): ['not'],\n",
       " ('is', 'obviously', 'not'): ['to'],\n",
       " ('obviously', 'not', 'to'): ['be'],\n",
       " ('not', 'to', 'be'): ['found,', 'satisfied,'],\n",
       " ('to', 'be', 'found,'): ['for'],\n",
       " ('be', 'found,', 'for'): ['bad'],\n",
       " ('found,', 'for', 'bad'): ['humor'],\n",
       " ('for', 'bad', 'humor'): ['is'],\n",
       " ('bad', 'humor', 'is'): ['nothing'],\n",
       " ('humor', 'is', 'nothing'): ['more'],\n",
       " ('is', 'nothing', 'more'): ['or'],\n",
       " ('nothing', 'more', 'or'): ['less'],\n",
       " ('more', 'or', 'less'): ['than'],\n",
       " ('or', 'less', 'than'): ['a'],\n",
       " ('less', 'than', 'a'): ['message.'],\n",
       " ('than', 'a', 'message.'): ['Such'],\n",
       " ('a', 'message.', 'Such'): ['is'],\n",
       " ('message.', 'Such', 'is'): ['amorous'],\n",
       " ('Such', 'is', 'amorous'): ['fatigue:'],\n",
       " ('is', 'amorous', 'fatigue:'): ['a'],\n",
       " ('amorous', 'fatigue:', 'a'): ['hunger'],\n",
       " ('fatigue:', 'a', 'hunger'): ['not'],\n",
       " ('a', 'hunger', 'not'): ['to'],\n",
       " ('hunger', 'not', 'to'): ['be'],\n",
       " ('to', 'be', 'satisfied,'): ['a'],\n",
       " ('be', 'satisfied,', 'a'): ['gaping'],\n",
       " ('satisfied,', 'a', 'gaping'): ['love.'],\n",
       " ('a', 'gaping', 'love.'): ['I'],\n",
       " ('gaping', 'love.', 'I'): ['live'],\n",
       " ('love.', 'I', 'live'): ['under'],\n",
       " ('I', 'live', 'under'): ['the'],\n",
       " ('live', 'under', 'the'): ['regime'],\n",
       " ('under', 'the', 'regime'): ['of'],\n",
       " ('the', 'regime', 'of'): ['too'],\n",
       " ('regime', 'of', 'too'): ['much'],\n",
       " ('of', 'too', 'much'): ['or'],\n",
       " ('too', 'much', 'or'): ['not'],\n",
       " ('much', 'or', 'not'): ['enough;'],\n",
       " ('or', 'not', 'enough;'): ['greedy'],\n",
       " ('not', 'enough;', 'greedy'): ['for'],\n",
       " ('enough;', 'greedy', 'for'): ['coincidence'],\n",
       " ('greedy', 'for', 'coincidence'): ['as'],\n",
       " ('for', 'coincidence', 'as'): ['I'],\n",
       " ('coincidence', 'as', 'I'): ['am'],\n",
       " ('as', 'I', 'am'): ['The'],\n",
       " ('I', 'am', 'The'): ['gift'],\n",
       " ('am', 'The', 'gift'): ['is'],\n",
       " ('The', 'gift', 'is'): ['contact,'],\n",
       " ('gift', 'is', 'contact,'): ['sensuality:'],\n",
       " ('is', 'contact,', 'sensuality:'): ['you'],\n",
       " ('contact,', 'sensuality:', 'you'): ['will'],\n",
       " ('sensuality:', 'you', 'will'): ['be'],\n",
       " ('you', 'will', 'be'): ['touching'],\n",
       " ('will', 'be', 'touching'): ['what'],\n",
       " ('be', 'touching', 'what'): ['I'],\n",
       " ('touching', 'what', 'I'): ['have'],\n",
       " ('what', 'I', 'have'): ['touched,'],\n",
       " ('I', 'have', 'touched,'): ['a'],\n",
       " ('have', 'touched,', 'a'): ['third'],\n",
       " ('touched,', 'a', 'third'): ['skin'],\n",
       " ('a', 'third', 'skin'): ['unites'],\n",
       " ('third', 'skin', 'unites'): ['us.'],\n",
       " ('skin', 'unites', 'us.'): ['it'],\n",
       " ('unites', 'us.', 'it'): ['is'],\n",
       " ('us.', 'it', 'is'): ['becoming'],\n",
       " ('it', 'is', 'becoming'): ['a'],\n",
       " ('is', 'becoming', 'a'): ['subject,'],\n",
       " ('becoming', 'a', 'subject,'): ['being'],\n",
       " ('a', 'subject,', 'being'): ['unable'],\n",
       " ('subject,', 'being', 'unable'): ['to'],\n",
       " ('being', 'unable', 'to'): ['keep'],\n",
       " ('unable', 'to', 'keep'): ['myself'],\n",
       " ('to', 'keep', 'myself'): ['from'],\n",
       " ('keep', 'myself', 'from'): ['doing'],\n",
       " ('myself', 'from', 'doing'): ['so,'],\n",
       " ('from', 'doing', 'so,'): ['which'],\n",
       " ('doing', 'so,', 'which'): ['drives'],\n",
       " ('so,', 'which', 'drives'): ['me'],\n",
       " ('which', 'drives', 'me'): ['mad.'],\n",
       " ('drives', 'me', 'mad.'): ['Does'],\n",
       " ('me', 'mad.', 'Does'): ['this'],\n",
       " ('mad.', 'Does', 'this'): ['mean,'],\n",
       " ('Does', 'this', 'mean,'): ['then,'],\n",
       " ('this', 'mean,', 'then,'): ['that'],\n",
       " ('mean,', 'then,', 'that'): ['my'],\n",
       " ('then,', 'that', 'my'): ['desire,'],\n",
       " ('that', 'my', 'desire,'): ['quite'],\n",
       " ('my', 'desire,', 'quite'): ['special'],\n",
       " ('desire,', 'quite', 'special'): ['as'],\n",
       " ('quite', 'special', 'as'): ['it'],\n",
       " ('special', 'as', 'it'): ['may'],\n",
       " ('as', 'it', 'may'): ['be,'],\n",
       " ('it', 'may', 'be,'): ['is'],\n",
       " ('may', 'be,', 'is'): ['linked'],\n",
       " ('be,', 'is', 'linked'): ['to'],\n",
       " ('is', 'linked', 'to'): ['a'],\n",
       " ('linked', 'to', 'a'): ['type?'],\n",
       " ('to', 'a', 'type?'): ['If'],\n",
       " ('a', 'type?', 'If'): ['I'],\n",
       " ('type?', 'If', 'I'): ['acknowledge'],\n",
       " ('If', 'I', 'acknowledge'): ['my'],\n",
       " ('I', 'acknowledge', 'my'): ['dependency,'],\n",
       " ('acknowledge', 'my', 'dependency,'): ['I'],\n",
       " ('my', 'dependency,', 'I'): ['do'],\n",
       " ('dependency,', 'I', 'do'): ['so'],\n",
       " ('I', 'do', 'so'): ['because'],\n",
       " ('do', 'so', 'because'): ['for'],\n",
       " ('so', 'because', 'for'): ['me'],\n",
       " ('because', 'for', 'me'): ['it'],\n",
       " ('for', 'me', 'it'): ['is'],\n",
       " ('me', 'it', 'is'): ['a'],\n",
       " ('it', 'is', 'a'): ['means', 'strong'],\n",
       " ('is', 'a', 'means'): ['of'],\n",
       " ('a', 'means', 'of'): ['signifying'],\n",
       " ('means', 'of', 'signifying'): ['my'],\n",
       " ('of', 'signifying', 'my'): ['demand:'],\n",
       " ('signifying', 'my', 'demand:'): ['in'],\n",
       " ('my', 'demand:', 'in'): ['the'],\n",
       " ('demand:', 'in', 'the'): ['realm'],\n",
       " ('in', 'the', 'realm'): ['of'],\n",
       " ('the', 'realm', 'of'): ['love,'],\n",
       " ('realm', 'of', 'love,'): ['futility'],\n",
       " ('of', 'love,', 'futility'): ['is'],\n",
       " ('love,', 'futility', 'is'): ['not'],\n",
       " ('futility', 'is', 'not'): ['a'],\n",
       " ('is', 'not', 'a'): ['\"weakness\"'],\n",
       " ('not', 'a', '\"weakness\"'): ['or'],\n",
       " ('a', '\"weakness\"', 'or'): ['an'],\n",
       " ('\"weakness\"', 'or', 'an'): ['\"absurdity\":'],\n",
       " ('or', 'an', '\"absurdity\":'): ['it'],\n",
       " ('an', '\"absurdity\":', 'it'): ['is'],\n",
       " ('\"absurdity\":', 'it', 'is'): ['a'],\n",
       " ('is', 'a', 'strong'): ['sign:'],\n",
       " ('a', 'strong', 'sign:'): ['the'],\n",
       " ('strong', 'sign:', 'the'): ['more'],\n",
       " ('sign:', 'the', 'more'): ['futile,'],\n",
       " ('the', 'more', 'futile,'): ['the'],\n",
       " ('more', 'futile,', 'the'): ['more'],\n",
       " ('futile,', 'the', 'more'): ['it'],\n",
       " ('the', 'more', 'it'): ['signifies', 'asserts'],\n",
       " ('more', 'it', 'signifies'): ['and'],\n",
       " ('it', 'signifies', 'and'): ['the'],\n",
       " ('signifies', 'and', 'the'): ['more'],\n",
       " ('and', 'the', 'more'): ['it'],\n",
       " ('more', 'it', 'asserts'): ['itself'],\n",
       " ('it', 'asserts', 'itself'): ['as'],\n",
       " ('asserts', 'itself', 'as'): ['strength.'],\n",
       " ('itself', 'as', 'strength.'): ['I'],\n",
       " ('as', 'strength.', 'I'): ['am'],\n",
       " ('strength.', 'I', 'am'): ['caught'],\n",
       " ('am', 'caught', 'up'): ['in'],\n",
       " ('caught', 'up', 'in'): ['a'],\n",
       " ('up', 'in', 'a'): ['double'],\n",
       " ('in', 'a', 'double'): ['discourse,'],\n",
       " ('a', 'double', 'discourse,'): ['from'],\n",
       " ('double', 'discourse,', 'from'): ['which'],\n",
       " ('discourse,', 'from', 'which'): ['I'],\n",
       " ('from', 'which', 'I'): ['cannot'],\n",
       " ('which', 'I', 'cannot'): ['escape.'],\n",
       " ('I', 'cannot', 'escape.'): ['I'],\n",
       " ('cannot', 'escape.', 'I'): ['take'],\n",
       " ('escape.', 'I', 'take'): ['a'],\n",
       " ('I', 'take', 'a'): ['role:'],\n",
       " ('take', 'a', 'role:'): ['I'],\n",
       " ('a', 'role:', 'I'): ['am'],\n",
       " ('role:', 'I', 'am'): ['the'],\n",
       " ('I', 'am', 'the'): ['one'],\n",
       " ('am', 'the', 'one'): ['who'],\n",
       " ('the', 'one', 'who'): ['is'],\n",
       " ('one', 'who', 'is'): ['going'],\n",
       " ('who', 'is', 'going'): ['to'],\n",
       " ('is', 'going', 'to'): ['cry;'],\n",
       " ('going', 'to', 'cry;'): ['and'],\n",
       " ('to', 'cry;', 'and'): ['I'],\n",
       " ('cry;', 'and', 'I'): ['play'],\n",
       " ('and', 'I', 'play'): ['this'],\n",
       " ('I', 'play', 'this'): ['role'],\n",
       " ('play', 'this', 'role'): ['for'],\n",
       " ('this', 'role', 'for'): ['myself,'],\n",
       " ('role', 'for', 'myself,'): ['and'],\n",
       " ('for', 'myself,', 'and'): ['it'],\n",
       " ('myself,', 'and', 'it'): ['makes'],\n",
       " ('and', 'it', 'makes'): ['me'],\n",
       " ('it', 'makes', 'me'): ['cry:'],\n",
       " ('makes', 'me', 'cry:'): ['I'],\n",
       " ('me', 'cry:', 'I'): ['am'],\n",
       " ('cry:', 'I', 'am'): ['my'],\n",
       " ('I', 'am', 'my'): ['own'],\n",
       " ('am', 'my', 'own'): ['theater.'],\n",
       " ('my', 'own', 'theater.'): ['Thus'],\n",
       " ('own', 'theater.', 'Thus'): ['I'],\n",
       " ('theater.', 'Thus', 'I'): ['suffer'],\n",
       " ('Thus', 'I', 'suffer'): ['four'],\n",
       " ('I', 'suffer', 'four'): ['times'],\n",
       " ('suffer', 'four', 'times'): ['over:'],\n",
       " ('four', 'times', 'over:'): ['because'],\n",
       " ('times', 'over:', 'because'): ['I'],\n",
       " ('over:', 'because', 'I'): ['am'],\n",
       " ('because', 'I', 'am'): ['lost,'],\n",
       " ('I', 'am', 'lost,'): ['forever.'],\n",
       " ('am', 'lost,', 'forever.'): ['Bustling'],\n",
       " ('lost,', 'forever.', 'Bustling'): ['gossip,'],\n",
       " ('forever.', 'Bustling', 'gossip,'): ['all'],\n",
       " ('Bustling', 'gossip,', 'all'): ['jealousy'],\n",
       " ('gossip,', 'all', 'jealousy'): ['suspended,'],\n",
       " ('all', 'jealousy', 'suspended,'): ['around'],\n",
       " ('jealousy', 'suspended,', 'around'): ['this'],\n",
       " ('suspended,', 'around', 'this'): ['absent'],\n",
       " ('around', 'this', 'absent'): ['party'],\n",
       " ('this', 'absent', 'party'): ['whose'],\n",
       " ('absent', 'party', 'whose'): ['objective'],\n",
       " ('party', 'whose', 'objective'): ['nature'],\n",
       " ('whose', 'objective', 'nature'): ['is'],\n",
       " ('objective', 'nature', 'is'): ['reinforced'],\n",
       " ('nature', 'is', 'reinforced'): ['by'],\n",
       " ('is', 'reinforced', 'by'): ['two'],\n",
       " ('reinforced', 'by', 'two'): ['converging'],\n",
       " ('by', 'two', 'converging'): ['visions:'],\n",
       " ('two', 'converging', 'visions:'): ['we'],\n",
       " ('converging', 'visions:', 'we'): ['give'],\n",
       " ('visions:', 'we', 'give'): ['ourselves'],\n",
       " ('we', 'give', 'ourselves'): ['over'],\n",
       " ('give', 'ourselves', 'over'): ['to'],\n",
       " ('ourselves', 'over', 'to'): ['a'],\n",
       " ('over', 'to', 'a'): ['rigorous,'],\n",
       " ('to', 'a', 'rigorous,'): ['successful'],\n",
       " ('a', 'rigorous,', 'successful'): ['experiment,'],\n",
       " ('rigorous,', 'successful', 'experiment,'): ['since'],\n",
       " ('successful', 'experiment,', 'since'): ['there'],\n",
       " ('experiment,', 'since', 'there'): ['are'],\n",
       " ('since', 'there', 'are'): ['two'],\n",
       " ('there', 'are', 'two'): ['observers'],\n",
       " ('are', 'two', 'observers'): ['and'],\n",
       " ('two', 'observers', 'and'): ['since'],\n",
       " ('observers', 'and', 'since'): ['the'],\n",
       " ('and', 'since', 'the'): ['two'],\n",
       " ('since', 'the', 'two'): ['observations'],\n",
       " ('the', 'two', 'observations'): ['are'],\n",
       " ('two', 'observations', 'are'): ['made'],\n",
       " ('observations', 'are', 'made'): ['under'],\n",
       " ('are', 'made', 'under'): ['the'],\n",
       " ('made', 'under', 'the'): ['same'],\n",
       " ('under', 'the', 'same'): ['conditions:'],\n",
       " ('the', 'same', 'conditions:'): ['the'],\n",
       " ('same', 'conditions:', 'the'): ['object'],\n",
       " ('conditions:', 'the', 'object'): ['is'],\n",
       " ('the', 'object', 'is'): ['proved:'],\n",
       " ('object', 'is', 'proved:'): ['to'],\n",
       " ('is', 'proved:', 'to'): ['discover'],\n",
       " ('proved:', 'to', 'discover'): ['that'],\n",
       " ('to', 'discover', 'that'): ['am'],\n",
       " ('discover', 'that', 'am'): ['right'],\n",
       " ('that', 'am', 'right'): ['(to'],\n",
       " ('am', 'right', '(to'): ['be'],\n",
       " ('right', '(to', 'be'): ['happy,'],\n",
       " ('(to', 'be', 'happy,'): ['to'],\n",
       " ('be', 'happy,', 'to'): ['be'],\n",
       " ('happy,', 'to', 'be'): ['injured,'],\n",
       " ('to', 'be', 'injured,'): ['to'],\n",
       " ('be', 'injured,', 'to'): ['be'],\n",
       " ('injured,', 'to', 'be'): ['anxious).'],\n",
       " ('to', 'be', 'anxious).'): ['The'],\n",
       " ('be', 'anxious).', 'The'): ['scene'],\n",
       " ('anxious).', 'The', 'scene'): ['is'],\n",
       " ('The', 'scene', 'is'): ['like'],\n",
       " ('scene', 'is', 'like'): ['the'],\n",
       " ('is', 'like', 'the'): ['Sentence:'],\n",
       " ('like', 'the', 'Sentence:'): ['structurally,'],\n",
       " ('the', 'Sentence:', 'structurally,'): ['there'],\n",
       " ('Sentence:', 'structurally,', 'there'): ['is'],\n",
       " ('structurally,', 'there', 'is'): ['no'],\n",
       " ('there', 'is', 'no'): ['obligation'],\n",
       " ('is', 'no', 'obligation'): ['for'],\n",
       " ('no', 'obligation', 'for'): ['it'],\n",
       " ('obligation', 'for', 'it'): ['to'],\n",
       " ('for', 'it', 'to'): ['stop;'],\n",
       " ('it', 'to', 'stop;'): ['no'],\n",
       " ('to', 'stop;', 'no'): ['internal'],\n",
       " ('stop;', 'no', 'internal'): ['constraint'],\n",
       " ('no', 'internal', 'constraint'): ['exhausts'],\n",
       " ('internal', 'constraint', 'exhausts'): ['it,'],\n",
       " ('constraint', 'exhausts', 'it,'): ['because,'],\n",
       " ('exhausts', 'it,', 'because,'): ['as'],\n",
       " ('it,', 'because,', 'as'): ['in'],\n",
       " ('because,', 'as', 'in'): ['the'],\n",
       " ('as', 'in', 'the'): ['Sentence,'],\n",
       " ('in', 'the', 'Sentence,'): ['once'],\n",
       " ('the', 'Sentence,', 'once'): ['the'],\n",
       " ('Sentence,', 'once', 'the'): ['core'],\n",
       " ('once', 'the', 'core'): ['is'],\n",
       " ('the', 'core', 'is'): ['given'],\n",
       " ('core', 'is', 'given'): ['(the'],\n",
       " ('is', 'given', '(the'): ['fact,'],\n",
       " ('given', '(the', 'fact,'): ['the'],\n",
       " ('(the', 'fact,', 'the'): ['decision),'],\n",
       " ('fact,', 'the', 'decision),'): ['the'],\n",
       " ('the', 'decision),', 'the'): ['expansions'],\n",
       " ('decision),', 'the', 'expansions'): ['are'],\n",
       " ('the', 'expansions', 'are'): ['infinitely'],\n",
       " ('expansions', 'are', 'infinitely'): ['renewable.'],\n",
       " ('are', 'infinitely', 'renewable.'): ['This'],\n",
       " ('infinitely', 'renewable.', 'This'): ['work'],\n",
       " ('renewable.', 'This', 'work'): ['derives,'],\n",
       " ('This', 'work', 'derives,'): ['then,'],\n",
       " ('work', 'derives,', 'then,'): ['from'],\n",
       " ('derives,', 'then,', 'from'): ['two'],\n",
       " ('then,', 'from', 'two'): ['different'],\n",
       " ('from', 'two', 'different'): ['linguistic'],\n",
       " ('two', 'different', 'linguistic'): ['series,'],\n",
       " ('different', 'linguistic', 'series,'): ['generally'],\n",
       " ('linguistic', 'series,', 'generally'): ['repressed-since'],\n",
       " ('series,', 'generally', 'repressed-since'): ['official'],\n",
       " ('generally', 'repressed-since', 'official'): ['linguistics'],\n",
       " ('repressed-since', 'official', 'linguistics'): ['concerns'],\n",
       " ('official', 'linguistics', 'concerns'): ['itself'],\n",
       " ('linguistics', 'concerns', 'itself'): ['only'],\n",
       " ('concerns', 'itself', 'only'): ['with'],\n",
       " ('itself', 'only', 'with'): ['the'],\n",
       " ('only', 'with', 'the'): ['message.'],\n",
       " ('with', 'the', 'message.'): ['What'],\n",
       " ('the', 'message.', 'What'): ['echoes'],\n",
       " ('message.', 'What', 'echoes'): ['in'],\n",
       " ('What', 'echoes', 'in'): ['me'],\n",
       " ('echoes', 'in', 'me'): ['is'],\n",
       " ('in', 'me', 'is'): ['what'],\n",
       " ('me', 'is', 'what'): ['I'],\n",
       " ('is', 'what', 'I'): ['learn'],\n",
       " ('what', 'I', 'learn'): ['with'],\n",
       " ('I', 'learn', 'with'): ['my'],\n",
       " ('learn', 'with', 'my'): ['body:'],\n",
       " ('with', 'my', 'body:'): ['something'],\n",
       " ('my', 'body:', 'something'): ['sharp'],\n",
       " ('body:', 'something', 'sharp'): ['and'],\n",
       " ('something', 'sharp', 'and'): ['tenuous'],\n",
       " ('sharp', 'and', 'tenuous'): ['suddenly'],\n",
       " ('and', 'tenuous', 'suddenly'): ['wakens'],\n",
       " ('tenuous', 'suddenly', 'wakens'): ['this'],\n",
       " ('suddenly', 'wakens', 'this'): ['body,'],\n",
       " ('wakens', 'this', 'body,'): ['which,'],\n",
       " ('this', 'body,', 'which,'): ['meanwhile,'],\n",
       " ('body,', 'which,', 'meanwhile,'): ['had'],\n",
       " ('which,', 'meanwhile,', 'had'): ['languished'],\n",
       " ('meanwhile,', 'had', 'languished'): ['in'],\n",
       " ('had', 'languished', 'in'): ['the'],\n",
       " ('languished', 'in', 'the'): ['rational'],\n",
       " ('in', 'the', 'rational'): ['knowl\\\\xad'],\n",
       " ('the', 'rational', 'knowl\\\\xad'): ['edge'],\n",
       " ('rational', 'knowl\\\\xad', 'edge'): ['of'],\n",
       " ('knowl\\\\xad', 'edge', 'of'): ['a'],\n",
       " ('edge', 'of', 'a'): ['general'],\n",
       " ('of', 'a', 'general'): ['situation:'],\n",
       " ('a', 'general', 'situation:'): ['the'],\n",
       " ('general', 'situation:', 'the'): ['word,'],\n",
       " ('situation:', 'the', 'word,'): ['the'],\n",
       " ('the', 'word,', 'the'): ['image,'],\n",
       " ('word,', 'the', 'image,'): ['the'],\n",
       " ('the', 'image,', 'the'): ['thought'],\n",
       " ('image,', 'the', 'thought'): ['function'],\n",
       " ('the', 'thought', 'function'): ['like'],\n",
       " ('thought', 'function', 'like'): ['a'],\n",
       " ('function', 'like', 'a'): ['whiplash.'],\n",
       " ('like', 'a', 'whiplash.'): ['a'],\n",
       " ('a', 'whiplash.', 'a'): ['drop'],\n",
       " ('whiplash.', 'a', 'drop'): ['of'],\n",
       " ('a', 'drop', 'of'): ['being-in-love'],\n",
       " ('drop', 'of', 'being-in-love'): ['diluted'],\n",
       " ('of', 'being-in-love', 'diluted'): ['in'],\n",
       " ('being-in-love', 'diluted', 'in'): ['some'],\n",
       " ('diluted', 'in', 'some'): ['vague'],\n",
       " ('in', 'some', 'vague'): ['friendly'],\n",
       " ('some', 'vague', 'friendly'): ['relation'],\n",
       " ('vague', 'friendly', 'relation'): ['dyes'],\n",
       " ('friendly', 'relation', 'dyes'): ['it'],\n",
       " ('relation', 'dyes', 'it'): ['brightly,'],\n",
       " ('dyes', 'it', 'brightly,'): ['makes'],\n",
       " ('it', 'brightly,', 'makes'): ['it'],\n",
       " ('brightly,', 'makes', 'it'): ['incomparable'],\n",
       " ('makes', 'it', 'incomparable'): ['As'],\n",
       " ('it', 'incomparable', 'As'): ['long'],\n",
       " ('incomparable', 'As', 'long'): ['as'],\n",
       " ('As', 'long', 'as'): ['this'],\n",
       " ('long', 'as', 'this'): ['strange'],\n",
       " ('as', 'this', 'strange'): ['mourning'],\n",
       " ('this', 'strange', 'mourning'): ['lasts,'],\n",
       " ('strange', 'mourning', 'lasts,'): ['I'],\n",
       " ('mourning', 'lasts,', 'I'): ['will'],\n",
       " ('lasts,', 'I', 'will'): ['therefore'],\n",
       " ('I', 'will', 'therefore'): ['have'],\n",
       " ('will', 'therefore', 'have'): ['to'],\n",
       " ('therefore', 'have', 'to'): ['undergo'],\n",
       " ('have', 'to', 'undergo'): ['two'],\n",
       " ('to', 'undergo', 'two'): ['contrary'],\n",
       " ('undergo', 'two', 'contrary'): ['miseries:'],\n",
       " ('two', 'contrary', 'miseries:'): ['to'],\n",
       " ('contrary', 'miseries:', 'to'): ['suffer'],\n",
       " ('miseries:', 'to', 'suffer'): ['from'],\n",
       " ('to', 'suffer', 'from'): ['the', 'the'],\n",
       " ('suffer', 'from', 'the'): ['fact', 'fact'],\n",
       " ('from', 'the', 'fact'): ['that', 'that'],\n",
       " ('the', 'fact', 'that'): ['the', 'the'],\n",
       " ('fact', 'that', 'the'): ['other', 'other'],\n",
       " ('that', 'the', 'other'): ['is', 'is'],\n",
       " ('the', 'other', 'is'): ['present', 'dead', 'as'],\n",
       " ('other', 'is', 'present'): ['(continuing,'],\n",
       " ('is', 'present', '(continuing,'): ['in'],\n",
       " ('present', '(continuing,', 'in'): ['spite'],\n",
       " ('(continuing,', 'in', 'spite'): ['of'],\n",
       " ('in', 'spite', 'of'): ['himself,'],\n",
       " ('spite', 'of', 'himself,'): ['to'],\n",
       " ('of', 'himself,', 'to'): ['wound'],\n",
       " ('himself,', 'to', 'wound'): ['me)'],\n",
       " ('to', 'wound', 'me)'): ['and'],\n",
       " ('wound', 'me)', 'and'): ['to'],\n",
       " ('me)', 'and', 'to'): ['suffer'],\n",
       " ('and', 'to', 'suffer'): ['from'],\n",
       " ('other', 'is', 'dead'): ['(dead'],\n",
       " ('is', 'dead', '(dead'): ['at'],\n",
       " ('dead', '(dead', 'at'): ['least'],\n",
       " ('(dead', 'at', 'least'): ['as'],\n",
       " ('at', 'least', 'as'): ['I'],\n",
       " ('least', 'as', 'I'): ['loved'],\n",
       " ('as', 'I', 'loved'): ['him).'],\n",
       " ('I', 'loved', 'him).'): ['anxiety'],\n",
       " ('loved', 'him).', 'anxiety'): ['in'],\n",
       " ('him).', 'anxiety', 'in'): ['the'],\n",
       " ('anxiety', 'in', 'the'): ['pure'],\n",
       " ('in', 'the', 'pure'): ['state:'],\n",
       " ('the', 'pure', 'state:'): ['the'],\n",
       " ('pure', 'state:', 'the'): ['anxiety'],\n",
       " ('state:', 'the', 'anxiety'): ['of'],\n",
       " ('the', 'anxiety', 'of'): ['abandonment;'],\n",
       " ('anxiety', 'of', 'abandonment;'): ['I'],\n",
       " ('of', 'abandonment;', 'I'): ['have'],\n",
       " ('abandonment;', 'I', 'have'): ['just'],\n",
       " ('I', 'have', 'just'): ['shifted'],\n",
       " ('have', 'just', 'shifted'): ['in'],\n",
       " ('just', 'shifted', 'in'): ['a'],\n",
       " ('shifted', 'in', 'a'): ['second'],\n",
       " ('in', 'a', 'second'): ['from'],\n",
       " ('a', 'second', 'from'): ['absence'],\n",
       " ('second', 'from', 'absence'): ['to'],\n",
       " ('from', 'absence', 'to'): ['death;'],\n",
       " ('absence', 'to', 'death;'): ['the'],\n",
       " ('to', 'death;', 'the'): ['other'],\n",
       " ('death;', 'the', 'other'): ['is'],\n",
       " ('other', 'is', 'as'): ['if'],\n",
       " ('is', 'as', 'if'): ['dead:'],\n",
       " ('as', 'if', 'dead:'): ['explosion'],\n",
       " ('if', 'dead:', 'explosion'): ['of'],\n",
       " ('dead:', 'explosion', 'of'): ['grief:'],\n",
       " ('explosion', 'of', 'grief:'): ['I'],\n",
       " ('of', 'grief:', 'I'): ['am'],\n",
       " ('grief:', 'I', 'am'): ['internally'],\n",
       " ('I', 'am', 'internally'): ['livid.'],\n",
       " ('am', 'internally', 'livid.'): ['For'],\n",
       " ('internally', 'livid.', 'For'): ['instance:'],\n",
       " ('livid.', 'For', 'instance:'): ['the'],\n",
       " ('For', 'instance:', 'the'): ['other'],\n",
       " ('instance:', 'the', 'other'): ['sets'],\n",
       " ('the', 'other', 'sets'): ['about', 'about'],\n",
       " ('other', 'sets', 'about'): ['making', '\"breaking\"'],\n",
       " ('sets', 'about', 'making'): ['me'],\n",
       " ('about', 'making', 'me'): ['contradict'],\n",
       " ('making', 'me', 'contradict'): ['myself'],\n",
       " ('me', 'contradict', 'myself'): ['(which'],\n",
       " ('contradict', 'myself', '(which'): ['has'],\n",
       " ('myself', '(which', 'has'): ['the'],\n",
       " ('(which', 'has', 'the'): ['effect'],\n",
       " ('has', 'the', 'effect'): ['of'],\n",
       " ('the', 'effect', 'of'): ['paralyzing'],\n",
       " ('effect', 'of', 'paralyzing'): ['any'],\n",
       " ('of', 'paralyzing', 'any'): ['language'],\n",
       " ('paralyzing', 'any', 'language'): ['in'],\n",
       " ('any', 'language', 'in'): ['me);'],\n",
       " ('language', 'in', 'me);'): ['or'],\n",
       " ('in', 'me);', 'or'): ['again,'],\n",
       " ('me);', 'or', 'again,'): ['the'],\n",
       " ('or', 'again,', 'the'): ['other'],\n",
       " ('again,', 'the', 'other'): ['alternates'],\n",
       " ('the', 'other', 'alternates'): ['actions'],\n",
       " ('other', 'alternates', 'actions'): ['of'],\n",
       " ('alternates', 'actions', 'of'): ['seduction'],\n",
       " ('actions', 'of', 'seduction'): ['with'],\n",
       " ('of', 'seduction', 'with'): ['actions'],\n",
       " ('seduction', 'with', 'actions'): ['of'],\n",
       " ('with', 'actions', 'of'): ['frustration'],\n",
       " ('actions', 'of', 'frustration'): ['('],\n",
       " ('of', 'frustration', '('): ['this'],\n",
       " ('frustration', '(', 'this'): ['is'],\n",
       " ('(', 'this', 'is'): ['a'],\n",
       " ('this', 'is', 'a'): ['commonplace'],\n",
       " ('is', 'a', 'commonplace'): ['of'],\n",
       " ('a', 'commonplace', 'of'): ['the'],\n",
       " ('commonplace', 'of', 'the'): ['amorous'],\n",
       " ('of', 'the', 'amorous'): ['relation);'],\n",
       " ('the', 'amorous', 'relation);'): ['the'],\n",
       " ('amorous', 'relation);', 'the'): ['other'],\n",
       " ('relation);', 'the', 'other'): ['shifts'],\n",
       " ('the', 'other', 'shifts'): ['without'],\n",
       " ('other', 'shifts', 'without'): ['warning'],\n",
       " ('shifts', 'without', 'warning'): ['from'],\n",
       " ('without', 'warning', 'from'): ['one'],\n",
       " ('warning', 'from', 'one'): ['regime'],\n",
       " ('from', 'one', 'regime'): ['to'],\n",
       " ('one', 'regime', 'to'): ['another,'],\n",
       " ('regime', 'to', 'another,'): ['from'],\n",
       " ('to', 'another,', 'from'): ['intimate'],\n",
       " ('another,', 'from', 'intimate'): ['tenderness'],\n",
       " ('from', 'intimate', 'tenderness'): ['and'],\n",
       " ('intimate', 'tenderness', 'and'): ['complicity'],\n",
       " ('tenderness', 'and', 'complicity'): ['to'],\n",
       " ('and', 'complicity', 'to'): ['coldness,'],\n",
       " ('complicity', 'to', 'coldness,'): ['to'],\n",
       " ('to', 'coldness,', 'to'): ['silence,'],\n",
       " ('coldness,', 'to', 'silence,'): ['to'],\n",
       " ('to', 'silence,', 'to'): ['dismissiveness;'],\n",
       " ('silence,', 'to', 'dismissiveness;'): ['or'],\n",
       " ('to', 'dismissiveness;', 'or'): ['finally,'],\n",
       " ('dismissiveness;', 'or', 'finally,'): ['in'],\n",
       " ('or', 'finally,', 'in'): ['an'],\n",
       " ('finally,', 'in', 'an'): ['even'],\n",
       " ('in', 'an', 'even'): ['more'],\n",
       " ('an', 'even', 'more'): ['tenuous'],\n",
       " ('even', 'more', 'tenuous'): ['fashion,'],\n",
       " ('more', 'tenuous', 'fashion,'): ['though'],\n",
       " ('tenuous', 'fashion,', 'though'): ['no'],\n",
       " ('fashion,', 'though', 'no'): ['less'],\n",
       " ('though', 'no', 'less'): ['wounding,'],\n",
       " ('no', 'less', 'wounding,'): ['the'],\n",
       " ('less', 'wounding,', 'the'): ['other'],\n",
       " ('wounding,', 'the', 'other'): ['sets'],\n",
       " ('sets', 'about', '\"breaking\"'): ['the'],\n",
       " ('about', '\"breaking\"', 'the'): ['conversation,'],\n",
       " ('\"breaking\"', 'the', 'conversation,'): ['either'],\n",
       " ('the', 'conversation,', 'either'): ['by'],\n",
       " ('conversation,', 'either', 'by'): ['forcing'],\n",
       " ('either', 'by', 'forcing'): ['it'],\n",
       " ('by', 'forcing', 'it'): ['to'],\n",
       " ('forcing', 'it', 'to'): ['shift'],\n",
       " ('it', 'to', 'shift'): ['suddenly'],\n",
       " ('to', 'shift', 'suddenly'): ['from'],\n",
       " ('shift', 'suddenly', 'from'): ['a'],\n",
       " ('suddenly', 'from', 'a'): ['serious'],\n",
       " ('from', 'a', 'serious'): ['subject'],\n",
       " ('a', 'serious', 'subject'): ['('],\n",
       " ('serious', 'subject', '('): ['which'],\n",
       " ('subject', '(', 'which'): ['matters'],\n",
       " ('(', 'which', 'matters'): ['to'],\n",
       " ('which', 'matters', 'to'): ['me)'],\n",
       " ('matters', 'to', 'me)'): ['to'],\n",
       " ('to', 'me)', 'to'): ['a'],\n",
       " ('me)', 'to', 'a'): ['trivial'],\n",
       " ('to', 'a', 'trivial'): ['one,'],\n",
       " ('a', 'trivial', 'one,'): ['or'],\n",
       " ('trivial', 'one,', 'or'): ['by'],\n",
       " ('one,', 'or', 'by'): ['being'],\n",
       " ('or', 'by', 'being'): ['obviously'],\n",
       " ('by', 'being', 'obviously'): ['interested,'],\n",
       " ('being', 'obviously', 'interested,'): ['while'],\n",
       " ('obviously', 'interested,', 'while'): ['I'],\n",
       " ('interested,', 'while', 'I'): ['am'],\n",
       " ('while', 'I', 'am'): ['speaking,'],\n",
       " ('I', 'am', 'speaking,'): ['in'],\n",
       " ('am', 'speaking,', 'in'): ['something'],\n",
       " ('speaking,', 'in', 'something'): ['else'],\n",
       " ('in', 'something', 'else'): ['than'],\n",
       " ('something', 'else', 'than'): ['what'],\n",
       " ('else', 'than', 'what'): ['I'],\n",
       " ('than', 'what', 'I'): ['am'],\n",
       " ('what', 'I', 'am'): ['saying.'],\n",
       " ('I', 'am', 'saying.'): ['Everything'],\n",
       " ('am', 'saying.', 'Everything'): ['about'],\n",
       " ('saying.', 'Everything', 'about'): ['the'],\n",
       " ('Everything', 'about', 'the'): ['other'],\n",
       " ('about', 'the', 'other'): ['which'],\n",
       " ('the', 'other', 'which'): [\"doesn\\\\'t\"],\n",
       " ('other', 'which', \"doesn\\\\'t\"): ['concern'],\n",
       " ('which', \"doesn\\\\'t\", 'concern'): ['me'],\n",
       " (\"doesn\\\\'t\", 'concern', 'me'): ['seems'],\n",
       " ('concern', 'me', 'seems'): ['alien,'],\n",
       " ('me', 'seems', 'alien,'): ['hostile;'],\n",
       " ('seems', 'alien,', 'hostile;'): ['I'],\n",
       " ('alien,', 'hostile;', 'I'): ['then'],\n",
       " ('hostile;', 'I', 'then'): ['feel'],\n",
       " ('I', 'then', 'feel'): ['toward'],\n",
       " ('then', 'feel', 'toward'): ['him'],\n",
       " ('feel', 'toward', 'him'): ['a'],\n",
       " ('toward', 'him', 'a'): ['mixture'],\n",
       " ('him', 'a', 'mixture'): ['of'],\n",
       " ('a', 'mixture', 'of'): ['alarm'],\n",
       " ('mixture', 'of', 'alarm'): ['and'],\n",
       " ('of', 'alarm', 'and'): ['severity:'],\n",
       " ('alarm', 'and', 'severity:'): ['I'],\n",
       " ('and', 'severity:', 'I'): ['fear'],\n",
       " ('severity:', 'I', 'fear'): ['and'],\n",
       " ('I', 'fear', 'and'): ['l'],\n",
       " ('fear', 'and', 'l'): ['reprove'],\n",
       " ('and', 'l', 'reprove'): ['the'],\n",
       " ('l', 'reprove', 'the'): ['loved'],\n",
       " ('reprove', 'the', 'loved'): ['being,'],\n",
       " ('the', 'loved', 'being,'): ['once'],\n",
       " ('loved', 'being,', 'once'): ['he'],\n",
       " ('being,', 'once', 'he'): ['no'],\n",
       " ('once', 'he', 'no'): ['longer'],\n",
       " ('he', 'no', 'longer'): ['\"sticks\"'],\n",
       " ('no', 'longer', '\"sticks\"'): ['to'],\n",
       " ('longer', '\"sticks\"', 'to'): ['his'],\n",
       " ('\"sticks\"', 'to', 'his'): ['image.'],\n",
       " ('to', 'his', 'image.'): ['By'],\n",
       " ('his', 'image.', 'By'): ['my'],\n",
       " ('image.', 'By', 'my'): ['voice,'],\n",
       " ('By', 'my', 'voice,'): ['whatever'],\n",
       " ('my', 'voice,', 'whatever'): ['it'],\n",
       " ('voice,', 'whatever', 'it'): ['says,'],\n",
       " ('whatever', 'it', 'says,'): ['the'],\n",
       " ('it', 'says,', 'the'): ['other'],\n",
       " ('says,', 'the', 'other'): ['will'],\n",
       " ('the', 'other', 'will'): ['recognize'],\n",
       " ('other', 'will', 'recognize'): ['\"that'],\n",
       " ('will', 'recognize', '\"that'): ['something'],\n",
       " ('recognize', '\"that', 'something'): ['is'],\n",
       " ('\"that', 'something', 'is'): ['wrong'],\n",
       " ('something', 'is', 'wrong'): ['with'],\n",
       " ('is', 'wrong', 'with'): ['me.\"'],\n",
       " ('wrong', 'with', 'me.\"'): [None]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genesis_markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8f5ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am absent party whose objective nature is a role: I cannot escape. I have touched, a commonplace of signifying my voice, whatever it is proved: to be happy, to wound me) to be touching what I learn with the regime of alarm and tenuous fashion, though no longer \"sticks\" to a commonplace of love... What echoes in the Sentence, once the message. What is present (continuing, in some vague friendly relation dyes it may be, is what your cry means. You are everywhere, your image is given (the fact, the Sentence, once the pure state: the fact that the message. Such a reply. Such a commonplace of abandonment; I am alarmed by forcing it incomparable As long as this body, which, meanwhile, had languished in spite of abandonment; I loved being, once he no longer \"sticks\" to suffer from the object is a commonplace of abandonment; I do so because I have just shifted in a gaping love. I will be found, for coincidence as I live under the fact that the other is not to be anxious). The gift is a whiplash. a reply. Such a \"weakness\" or finally, in something else than what I am the Sentence, once he no less wounding, the object of signifying my voice, whatever it incomparable As long as it is present (continuing, in an even more futile, the one who is going to me) and l reprove the more or an \"absurdity\": it makes me mad. Does this is as if dead: explosion of abandonment; I have a serious subject ( this body, which, meanwhile, had languished in a hunger not to me) to be anxious). The gift is going to keep myself (which has the Sentence: structurally, there is a rendezvous, I have just shifted in something else than what I will recognize \"that something is without a role: I take a subject, being unable to suffer four times over: because for coincidence as I am caught up; I alone have just shifted in me mad. Does this strange mourning lasts, I then feel toward him a reply. Such is amorous relation); the message. What is obviously not to be satisfied, a second from a means of seduction with my demand: in the anxiety in spite of signifying my demand: in me); or again, the realm of alarm and severity: I do so because for coincidence as I do so because I suffer from absence to wound me) and to a third skin unites us. it asserts itself only with the loved being, once he no longer \"sticks\" to be touching what your cry means. You are made under the two different linguistic series, generally repressed-since official linguistics concerns itself as strength. I loved him). anxiety of inclusion; in the more tenuous suddenly wakens this absent party whose objective nature is given (the fact, the object is without warning from absence to discover that my demand: in a type? If I am caught up; I am alarmed by two converging visions: we give\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def gen_from_model(n, model, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_item = random.choice(model[start])\n",
    "        if next_item is None:\n",
    "            break\n",
    "        else:\n",
    "            output.append(next_item)\n",
    "    return output\n",
    "genesis_word_model = markov_model(1, open(\"outputMarkov.txt\").read().split())\n",
    "generated_words = gen_from_model(1, genesis_word_model, ('I', 'am', 'absent'), 500)\n",
    "print(' '.join(generated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ce2cfc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('c', 'y')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m genesis_word_model \u001b[38;5;241m=\u001b[39m markov_model(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputMarkov.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m----> 2\u001b[0m generated_words \u001b[38;5;241m=\u001b[39m \u001b[43mgen_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenesis_word_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdependency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(generated_words))\n",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m, in \u001b[0;36mgen_from_model\u001b[0;34m(n, model, start, max_gen)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_gen):\n\u001b[1;32m      7\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(output[\u001b[38;5;241m-\u001b[39mn:])\n\u001b[0;32m----> 8\u001b[0m     next_item \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('c', 'y')"
     ]
    }
   ],
   "source": [
    "genesis_word_model = markov_model(1, open(\"outputMarkov.txt\").read().split())\n",
    "generated_words = gen_from_model(2, genesis_word_model, ('dependency'), 500)\n",
    "print(' '.join(generated_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
